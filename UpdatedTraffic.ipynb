{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Updated Traffic from LTA News </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this project is to get the updated news from LTA twitter website to pinpoint the location of accident.<br />\n",
    "First, we will need to prepare the data from the twitter. I have use selenium to parse more tweets than given 1 page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport requests\\nfrom io import StringIO\\nfrom lxml import html\\nimport HTMLParser\\nfrom datetime import datetime\\n\\nwebpage = requests.get(\"https://twitter.com/ltatrafficnews\")\\ndoc = html.fromstring(webpage.content)\\ntweet_container = doc.xpath(\\'//p[@class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\"]/text()\\')\\n#print(tweet_container)\\ntemp = doc.xpath(\\'//span[contains(@class,\"_timestamp js-short-timestamp js-relative-timestamp\")]\\')\\ntemp = [t.get(\\'data-time\\') for t in temp]\\ntemp = [int(t) for t in temp]\\ntemp = [datetime.fromtimestamp(ts).strftime(\\'%Y-%m-%d %H:%M:%S\\') for ts in temp]\\n#print(temp)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "browser = webdriver.Chrome()\n",
    "url = \"https://twitter.com/ltatrafficnews\"\n",
    "\n",
    "browser.get(url)\n",
    "time.sleep(2)\n",
    "'''\n",
    "# when internet connection is good use the first way to scroll down\n",
    "SCROLL_PAUSE_TIME = 5\n",
    "\n",
    "\n",
    "# Get scroll height\n",
    "last_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "while True:\n",
    "    # Scroll down to bottom\n",
    "    browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "    # Wait to load page\n",
    "    time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "    # Calculate new scroll height and compare with last scroll height\n",
    "    new_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height\n",
    "'''\n",
    "body = browser.find_element_by_tag_name('body')\n",
    "for _ in range(1000):\n",
    "    body.send_keys(Keys.PAGE_DOWN)\n",
    "    time.sleep(1)\n",
    "    \n",
    "tweets = browser.find_elements_by_class_name('tweet-text')\n",
    "time_stamps = browser.find_elements_by_class_name('js-short-timestamp')\n",
    "\n",
    "t_list = []\n",
    "for tweet in tweets:\n",
    "    t_list.append(tweet.text)\n",
    "\n",
    "ts_list = []\n",
    "for ts in time_stamps:\n",
    "    ts_list.append(ts.get_attribute('data-time'))\n",
    "    \n",
    "browser.quit()\n",
    "\n",
    "'''\n",
    "import requests\n",
    "from io import StringIO\n",
    "from lxml import html\n",
    "import HTMLParser\n",
    "from datetime import datetime\n",
    "\n",
    "webpage = requests.get(\"https://twitter.com/ltatrafficnews\")\n",
    "doc = html.fromstring(webpage.content)\n",
    "tweet_container = doc.xpath('//p[@class=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\"]/text()')\n",
    "#print(tweet_container)\n",
    "temp = doc.xpath('//span[contains(@class,\"_timestamp js-short-timestamp js-relative-timestamp\")]')\n",
    "temp = [t.get('data-time') for t in temp]\n",
    "temp = [int(t) for t in temp]\n",
    "temp = [datetime.fromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S') for ts in temp]\n",
    "#print(temp)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are create a dataframe from the data that we have fetch off the twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully fetch data from twitter.\n",
      "                DateTime                                              Event\n",
      "0    2019-03-19 14:02:20  Accident on TPE (towards SLE) before Tampines ...\n",
      "1    2019-03-19 13:52:20  Obstacle on AYE (towards Tuas) after Tuas Rd E...\n",
      "2    2019-03-19 12:12:14  Accident on Dunearn Road (towards City) after ...\n",
      "3    2019-03-19 11:01:42  Accident on Lower Delta Road towards Jalan Buk...\n",
      "4    2019-03-19 10:39:32  Accident on PIE (towards Tuas) after Toa Payoh...\n",
      "5    2019-03-19 10:27:51  Accident on Stevens Road towards Balmoral Park...\n",
      "6    2019-03-19 09:54:32  Accident on Hill Street (towards Coleman Stree...\n",
      "7    2019-03-19 09:46:48  Accident on KPE (towards ECP) after Tampines R...\n",
      "8    2019-03-19 09:35:41  Accident on KPE (towards ECP) after Tampines R...\n",
      "9    2019-03-19 09:10:04  Accident on PIE (towards Tuas) after Thomson R...\n",
      "10   2019-03-19 09:00:05  Accident on PIE (towards Tuas) after Thomson R...\n",
      "11   2019-03-19 08:44:04  Accident on PIE (towards Tuas) after Thomson R...\n",
      "12   2019-03-19 08:39:38  Accident on PIE (towards Tuas) after Thomson R...\n",
      "13   2019-03-19 08:07:16  Obstacle on PIE (towards Changi Airport) at Lo...\n",
      "14   2019-03-18 22:37:16  Accident on KJE (towards BKE) after Choa Chu K...\n",
      "15   2019-03-18 22:33:23  Accident on Ang Mo Kio Avenue 5 (towards Ang M...\n",
      "16   2019-03-18 22:28:28  Accident on KJE (towards BKE) after Choa Chu K...\n",
      "17   2019-03-18 22:18:40  Accident on Ang Mo Kio Avenue 6 near Ang Mo Ki...\n",
      "18   2019-03-18 22:18:08  Accident on KJE (towards BKE) after Choa Chu K...\n",
      "19   2019-03-18 20:50:56  Obstacle on PIE (towards Changi Airport) after...\n",
      "20   2019-03-18 19:11:16  Obstacle on BKE (towards Woodlands) before KJE...\n",
      "21   2019-03-18 19:07:50  Obstacle on BKE (towards PIE) after KJE Exit. ...\n",
      "22   2019-03-18 18:54:32  Heavy Traffic on CTE (towards AYE) before PIE(...\n",
      "23   2019-03-18 18:53:26  Accident on CTE (towards AYE) before PIE(Chang...\n",
      "24   2019-03-18 18:45:05  Accident on KPE (towards TPE) after ECP Entran...\n",
      "25   2019-03-18 18:35:44  Accident on CTE (towards AYE) before PIE(Chang...\n",
      "26   2019-03-18 18:35:06  Accident on CTE (towards AYE) before PIE(Chang...\n",
      "27   2019-03-18 18:30:43  Accident on MCE (towards ECP) before ECP (Chan...\n",
      "28   2019-03-18 18:18:28  Accident on CTE (towards AYE) before PIE(Chang...\n",
      "29   2019-03-18 18:11:54  Obstacle on PIE (towards Tuas) after Toa Payoh...\n",
      "..                   ...                                                ...\n",
      "370  2019-03-11 15:14:55  Accident on CTE (towards AYE) before PIE(Chang...\n",
      "371  2019-03-11 15:13:37  Accident on CTE (towards AYE) before PIE(Chang...\n",
      "372  2019-03-11 11:26:53  Accident on AYE (towards MCE) at Clementi Ave ...\n",
      "373  2019-03-11 11:24:14  Accident on River Valley Road near Killiney Ro...\n",
      "374  2019-03-11 11:21:03  Accident on SLE (towards CTE) before Upper Tho...\n",
      "375  2019-03-11 10:39:49  Accident on AYE (towards MCE) after Jurong Tow...\n",
      "376  2019-03-11 09:55:22  Accident on KPE (towards ECP) before Airport R...\n",
      "377  2019-03-11 09:44:13  Accident on AYE (towards Tuas) after Alexandra...\n",
      "378  2019-03-11 09:39:37  Accident on AYE (towards Tuas) after Alexandra...\n",
      "379  2019-03-11 09:04:17  Accident on AYE (towards Tuas) after Lower Del...\n",
      "380  2019-03-11 09:02:22  Accident on Upper Serangoon Road (towards City...\n",
      "381  2019-03-11 08:52:41  Heavy Traffic on AYE (towards Tuas) after Lowe...\n",
      "382  2019-03-11 08:39:40  Obstacle on KJE (towards PIE) after Sungei Ten...\n",
      "383  2019-03-11 08:33:51  Accident on AYE (towards Tuas) after Lower Del...\n",
      "384  2019-03-11 08:32:34  Accident on AYE (towards Tuas) after Lower Del...\n",
      "385  2019-03-11 08:13:46  Obstacle on PIE (towards Changi Airport) after...\n",
      "386  2019-03-11 08:08:33  Accident on Adam Road (towards Queensway) afte...\n",
      "387  2019-03-11 08:08:33  Obstacle on PIE (towards Tuas) after Toa Payoh...\n",
      "388  2019-03-11 07:51:43  Accident on Geylang Road (toward City) after J...\n",
      "389  2019-03-11 07:23:09  Accident on PIE (towards Tuas) after KPE Exit ...\n",
      "390  2019-03-11 07:21:15  Accident on PIE (towards Tuas) after Bedok Nor...\n",
      "391  2019-03-11 07:19:14  Accident on PIE (towards Tuas) after KPE Exit....\n",
      "392  2019-03-11 07:14:02  Accident on Tuas West Road (towards Tuas Link ...\n",
      "393  2019-03-11 06:38:16  Accident on Holland Road (towards Lorong Mambo...\n",
      "394  2019-03-11 06:20:15  Accident on Holland Road near Holland Avenue/H...\n",
      "395  2019-03-11 04:02:59  Accident on PIE (towards Changi Airport) at To...\n",
      "396  2019-03-10 22:50:51  Heavy Traffic on PIE (towards Tuas) after Stev...\n",
      "397  2019-03-10 22:36:59  Accident on PIE (towards Tuas) after Stevens R...\n",
      "398  2019-03-10 22:34:16  Accident on PIE (towards Tuas) after Stevens R...\n",
      "399  2019-03-10 17:57:58  Obstacle on PIE (towards Tuas) after Upper Cha...\n",
      "\n",
      "[400 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def formfetch(tweets,timestamps):\n",
    "    if len(tweets) == 0 or len(timestamps) == 0:\n",
    "        print(\"There's no data successfully fetched from chrome, check the driver and element of webpage.\")\n",
    "        df = None\n",
    "    else:\n",
    "        tweets = [t.encode('utf-8') for t in tweets]\n",
    "        times = [int(t) for t in timestamps]\n",
    "        times = [datetime.fromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S') for ts in times]\n",
    "        df = pd.DataFrame(np.column_stack([times,tweets]),columns=['DateTime','Event'])\n",
    "        #df.to_csv(\"test_csv.csv\",encoding='utf-8',index=False)\n",
    "        print(\"Successfully fetch data from twitter.\")\n",
    "    return df\n",
    "\n",
    "df_fetch = formfetch(t_list,ts_list)\n",
    "print(df_fetch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to update the newly fetch data into the total data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Database length is 1202.\n"
     ]
    }
   ],
   "source": [
    "df_total = pd.read_csv(\"total_data.csv\")\n",
    "def updateData(_totaldf,_fetchdf):\n",
    "    _cut_off = (_totaldf.loc[0]['DateTime'],_totaldf.loc[0]['Event'])\n",
    "    _id_to_cut = _fetchdf[(_fetchdf['DateTime']==_cut_off[0])&(_fetchdf['Event']==_cut_off[1])].index.astype(int)[0]\n",
    "    _pdf = _fetchdf.loc[0:_id_to_cut-2]\n",
    "    _newdf = _totaldf.append(_pdf, sort=True)\n",
    "    _newdf = _newdf.sort_values(\"DateTime\",ascending=False)\n",
    "    _newdf = _newdf.reset_index(drop=True)\n",
    "    if len(_pdf)+len(_totaldf) == len(_newdf) :\n",
    "        _newdf.to_csv(\"total_data.csv\",encoding='utf-8',index=False)\n",
    "        print(\"Total Database length is {}.\".format(len(_newdf)))\n",
    "    else:\n",
    "        print(\"There's something wrong in updating dataframe, please check!\")\n",
    "    return _newdf\n",
    "\n",
    "df = updateData(df_total,df_fetch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we have already the data set, we will need to do some auto tagging. We will need to perform sentence cleaning and also extract respective address keyword in the events. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "stopwords = [\"near\",\"/\",\"along\",\"between\",\"end\",\"on\",\"towards\",\"before\",\"after\",\"avoid\",\"with\",\"congestion\",\"till\",\"exit\",\"exit.\",\"closed\",\"entrance\",\"entrance.\",\"at\",\"right.\",\"right\",\"tunnel\",'.']\n",
    "\n",
    "def containWord(s,w):\n",
    "    if (' '+w+' ') in (' '+s+' '):\n",
    "        res = True\n",
    "    elif (w+' ') in (' '+s+' '):\n",
    "        res = True\n",
    "    elif (w+' ') in (' '+s+' '):\n",
    "        res = True\n",
    "    else:\n",
    "        res = False\n",
    "    return res\n",
    "\n",
    "def clean_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r'[^a-z0-9\\s]', '', sentence)\n",
    "    return re.sub(r'\\s{2,}',' ', sentence)\n",
    "\n",
    "def keywordExt(s):\n",
    "    add_l = []\n",
    "    l_w = s.split(\" \")\n",
    "    ii = 0\n",
    "    while (l_w[ii] != 'end'):\n",
    "        if l_w[ii] in stopwords:\n",
    "            add_e = \"\"\n",
    "            while l_w[ii+1] not in stopwords:\n",
    "                if len(add_e) == 0:\n",
    "                    add_e = add_e+l_w[ii+1]\n",
    "                else:\n",
    "                    add_e = add_e+\" \"+l_w[ii+1]\n",
    "                ii += 1\n",
    "            if len(add_e) != 0 :\n",
    "                add_l.append(add_e)\n",
    "        ii += 1\n",
    "    return add_l\n",
    "\n",
    "def generateAdd(df):\n",
    "    events = df['Event'].tolist()\n",
    "    sep = \"#\"\n",
    "    events = [ s.split(sep,1)[0] for s in events]\n",
    "    events = [ clean_sentence(e) for e in events]\n",
    "    addlist = [keywordExt(string+\" end\") for string in events]\n",
    "    cleanlist = []\n",
    "    for ii in range(len(addlist)):\n",
    "        jj = 0\n",
    "        add = []\n",
    "        for jj in range(len(addlist[ii])):\n",
    "            if not(containWord(addlist[ii][jj],'lane') or containWord(addlist[ii][jj],'lanes')):\n",
    "                if addlist[ii][jj] == 'piechangi':\n",
    "                    add.append('pie changi')\n",
    "                elif addlist[ii][jj] == 'piejurong':\n",
    "                    add.append('pie jurong')\n",
    "                elif containWord(addlist[ii][jj],'amk'):\n",
    "                    add.append('ang mo kio '+addlist[ii][jj].split(\" \",1)[1])\n",
    "                elif containWord(addlist[ii][jj],'tg'):\n",
    "                    add.append('tanjong '+addlist[ii][jj].split(\" \",1)[1])    \n",
    "                else:\n",
    "                    add.append(addlist[ii][jj])\n",
    "            jj += 1\n",
    "        cleanlist.append(add)\n",
    "    # if lane is need, add addlist as address instead of cleanlist\n",
    "    df['address'] = cleanlist\n",
    "    #df.to_csv(\"test.csv\",index=False)\n",
    "    return df\n",
    "\n",
    "df = generateAdd(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully found AddressBook and it's loaded!\n",
      "The length of address book is 281.\n"
     ]
    }
   ],
   "source": [
    "from geopy.geocoders import Nominatim,BANFrance\n",
    "from geopy.exc import GeocoderTimedOut\n",
    "addlist = df['address'].tolist()\n",
    "def createAddplusCoor(addresslist):\n",
    "    _add_book = dict()\n",
    "    geolocator = Nominatim(user_agent=\"jimmy-app\",timeout=2000)\n",
    "    for ii in range(len(addresslist)):\n",
    "        for jj in range(len(addresslist[ii])):\n",
    "            try:\n",
    "                _add_book[addresslist[ii][jj]]\n",
    "            except KeyError:\n",
    "                _loc = geolocator.geocode(addresslist[ii][jj]+\" singapore\")\n",
    "                if _loc != None:\n",
    "                    _add_book[addresslist[ii][jj]] = (_loc.latitude,_loc.longitude)\n",
    "    return _add_book\n",
    "\n",
    "try:\n",
    "    ab = pd.read_csv(\"AddressBook.csv\",index_col=0)\n",
    "    address_book = dict([(i,(a,b)) for i,a,b in zip(ab.index,ab.latitude,ab.longitude)])\n",
    "    print(\"Successfully found AddressBook and it's loaded!\")\n",
    "except IOError:\n",
    "    print(\"Error! Address Book not found, proceed to create address book. Please wait!\")\n",
    "    address_book = createAddplusCoor(addlist)\n",
    "    ab = pd.DataFrame.from_dict(address_book, orient='index', columns=['latitude','longitude'])\n",
    "    ab.to_csv(\"AddressBook.csv\")\n",
    "\n",
    "assert(isinstance(address_book,dict))\n",
    "print(\"The length of address book is {}.\".format(len(address_book)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I proceed to assign coordinates to each event based on their keyword address. New address not in the address book will be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There's no coordinate found for ctesle.\n",
      "There's no coordinate found for ang mo kio avenue 5ang mo kio avenue 6 junction.\n",
      "There's no coordinate found for sims way junction.\n",
      "There's no coordinate found for chin swee roadclemenceau avenue junction.\n",
      "There's no coordinate found for raffles boulevardtemasek avenue junction.\n",
      "There's no coordinate found for clemenceau avenueriver valley road junction.\n",
      "There's no coordinate found for pioneer rd nth.\n",
      "There's no coordinate found for braddell rdlor chuan.\n",
      "There's no coordinate found for pietuas.\n",
      "There's no coordinate found for dunearn underpass.\n",
      "There's no coordinate found for farrer flyover.\n",
      "There's no coordinate found for 19 21 mar 2019 from midnight to 5 am.\n",
      "There's no coordinate found for friday 15 march 2019.\n",
      "There's no coordinate found for 10 pm.\n",
      "There's no coordinate found for sunday 24 march 2019.\n",
      "There's no coordinate found for wednesday 20 march 2019 and thursday 21 march 2019 from 130 am to 5 am daily.\n",
      "There's no coordinate found for yuan ching road tao ching road chinese garden road.\n",
      "There's no coordinate found for tuesday 19 march 2019.\n",
      "There's no coordinate found for 4 pm.\n",
      "There's no coordinate found for balestier roadthomson road junction.\n",
      "There's no coordinate found for tah ching road chinese garden road yuan ching road.\n",
      "There's no coordinate found for kpe tpe.\n",
      "There's no coordinate found for saturday 16 march 2019 and sunday 17 march 2019 from midnight to 5 am daily.\n",
      "There's no coordinate found for from monday 18 march 2019 to wednesday 20 march 2019 from midnight to 6 am daily.\n",
      "There's no coordinate found for bkepie.\n",
      "There's no coordinate found for tpepie.\n",
      "There's no coordinate found for east coast park carpark f3.\n",
      "There's no coordinate found for kjepie.\n",
      "There's no coordinate found for killiney roadriver valley close junction.\n",
      "There's no coordinate found for geylang road toward city.\n",
      "There's no coordinate found for holland avenueholland road junction.\n",
      "There's no coordinate found for toa payoh kim keat link.\n",
      "There's no coordinate found for ctecity.\n",
      "There's no coordinate found for slebke.\n",
      "There's no coordinate found for gambas avenuewoodlands avenue 8 junction.\n",
      "There's no coordinate found for sengkang east roadsengkang east way junction.\n",
      "There's no coordinate found for toa payoh lor 1.\n",
      "There's no coordinate found for 1214 mar and 26 28 mar 2019 from midnight to 5 am daily.\n",
      "There's no coordinate found for 89 11 27 29 mar 2019 from midnight to 5 am.\n",
      "There's no coordinate found for 8 12 14 18 2526 28 mar and 1 apr 2019 from midnight to 5 am.\n",
      "There's no coordinate found for corporation roadjurong west avenue 2 junction.\n",
      "There's no coordinate found for bedok north avenue 1bedok north road junction.\n",
      "There's no coordinate found for upper changi road east toward city.\n",
      "There's no coordinate found for saturday 2 mar 2019 from midnight to 5 am.\n",
      "There's no coordinate found for beach roadrochor road junction.\n",
      "There's no coordinate found for mon 4 mar 2019 from midnight to 5 am.\n",
      "There's no coordinate found for mon 4 mar 2019 and tues 5 mar 2019 from midnight to 5 am.\n",
      "There's no coordinate found for 7 to kpe mce from tues 5 mar 2019 to sat 9 mar 2019 from 1 am to 5 am daily.\n",
      "There's no coordinate found for 6 to kpe ecp from sun 3 mar 2019 to wed 6 mar 2019 from 1230 am to 5 am daily.\n",
      "There's no coordinate found for tpesle.\n",
      "There's no coordinate found for clarke quayriver valley road junction.\n",
      "There's no coordinate found for mandai roadsle junction.\n",
      "There's no coordinate found for yishun avenue 2yishun avenue 5 junction.\n",
      "There's no coordinate found for t1 vip drive.\n",
      "There's no coordinate found for mceaye.\n",
      "There's no coordinate found for bkewoodlands.\n",
      "There's no coordinate found for simei avenueupper changi road east junction.\n",
      "There's no coordinate found for jalan sultanrochor canal road junction.\n",
      "There's no coordinate found for grange roadorchard boulevard junction.\n",
      "There's no coordinate found for loyang avenuepasir ris drive 1 junction.\n",
      "There's no coordinate found for tuas south avenue 2tuas south avenue 3 junction.\n",
      "There's no coordinate found for thurs 28 feb 2019 from midnight to 5 am.\n",
      "There's no coordinate found for wed 27 feb 2019 and thurs 28 feb 2019 from 1 am to 5 am daily.\n",
      "There's no coordinate found for wed 27 feb 2019 thurs 28 feb 2019 from midnight to 5 am.\n",
      "There's no coordinate found for sun 24 feb 2019 from 130 am to 5 am.\n",
      "There's no coordinate found for wed 27 feb 2019 from 1 am to 5 am.\n",
      "There's 108 number of error address in the data.\n"
     ]
    }
   ],
   "source": [
    "#print(df.head())\n",
    "import numpy as np\n",
    "\n",
    "def coordinateTagging(df,address_book):\n",
    "    addlist = df['address'].tolist()\n",
    "    geolocator = Nominatim(user_agent=\"jimmy-app\",timeout=2000)\n",
    "    cord_l = []\n",
    "    n_error = 0\n",
    "    error_list = []\n",
    "    for ii in range(len(addlist)):\n",
    "        coord_e = []\n",
    "        for jj in range(len(addlist[ii])):\n",
    "            try:\n",
    "                coord_e.append(address_book[addlist[ii][jj]])\n",
    "            except KeyError:\n",
    "                _loc = geolocator.geocode(addlist[ii][jj]+\" singapore\")\n",
    "                if _loc != None:\n",
    "                    address_book[addlist[ii][jj]] = (_loc.latitude,_loc.longitude)\n",
    "                    coord_e.append(address_book[addlist[ii][jj]])\n",
    "                else:\n",
    "                    n_error += 1\n",
    "                    if not(addlist[ii][jj] in error_list):\n",
    "                        print(\"There's no coordinate found for {}.\".format(addlist[ii][jj]))\n",
    "                        error_list.append(addlist[ii][jj])\n",
    "                    coord_e.append((np.nan,np.nan))\n",
    "                    \n",
    "        cord_l.append(coord_e)\n",
    "    ab = pd.DataFrame.from_dict(address_book, orient='index', columns=['latitude','longitude'])\n",
    "    ab.to_csv(\"AddressBook.csv\")\n",
    "    df['coordinate'] = cord_l\n",
    "    return df,address_book,n_error\n",
    "\n",
    "Fdf,new_address_book,error = coordinateTagging(df,address_book)\n",
    "\n",
    "Fdf.to_csv(\"tagged.csv\",encoding='utf-8',index=False)\n",
    "print(\"There's {} number of error address in the data.\".format(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
